{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: huggingface in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: datasets in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: seaborn in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: wandb in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (0.18.6)\n",
      "Requirement already satisfied: torch in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: six>=1.10 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (4.67.0)\n",
      "Requirement already satisfied: python-slugify in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: filelock in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from wandb) (75.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from requests->kaggle) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: webencodings in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\workspace\\leetcode-workspace\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kaggle huggingface datasets seaborn matplotlib wandb torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Token under section \"Access Tokens\" in your Hugging Face profile\n",
    "login(token=\"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\") \n",
    "\n",
    "# Create account on wandb.ai and get the API key from https://wandb.ai/{YOUR_USERNAME}\n",
    "os.environ[\"WANDB_API_KEY\"]=\"59f6935e0f9107148258c6fb79e915cd8a4acd22\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Fine-tuning bert-base-uncased\"\n",
    "os.environ[\"WANDB_NAME\"] = \"ft-bert-base-uncased-for-binary-search\"\n",
    "os.environ[\"MODEL_NAME\"] = \"bert-base-uncased\"\n",
    "os.environ[\"TOKENIZER_NAME\"] = \"bert-base-uncased\"\n",
    "os.environ[\"DATASET\"] = \"https://www.kaggle.com/datasets/skywardai/network-vulnerability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Text', 'Label', 'Vulnerability Type', 'Timestamp'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dict = load_dataset('csv', data_files='kaggle/input/network_vulnerability_dataset.csv')\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Text', 'Label', 'Vulnerability Type', 'Timestamp'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'Text', 'Label', 'Vulnerability Type', 'Timestamp'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = dataset_dict.get('train')\n",
    "\n",
    "pre_processed_ds=dataset_dict.train_test_split(test_size=0.2)\n",
    "\n",
    "pre_processed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_ds_train_low = pre_processed_ds['train'].shuffle(seed=42).select(range(800))\n",
    "pre_processed_ds_test_low = pre_processed_ds['test'].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer (e.g., BERT tokenizer)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8):   0%|          | 0/800 [00:04<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"d:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3458, in _map_single\n    batch = apply_function_on_filtered_inputs(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3320, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\minhq\\AppData\\Local\\Temp\\ipykernel_2960\\3327773787.py\", line 4, in preprocess_function\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply the tokenizer to the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tokenized_dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mpre_processed_ds_train_low\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m tokenized_dataset_test \u001b[38;5;241m=\u001b[39m pre_processed_ds_test_low\u001b[38;5;241m.\u001b[39mmap(preprocess_function, num_proc \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32md:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3147\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3141\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3143\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3144\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3145\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3146\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3151\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32md:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32md:\\workspace\\leetcode-workspace\\venv\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_inputs = tokenizer(examples['Text'], padding=\"max_length\", truncation=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "\n",
    "tokenized_dataset_train = pre_processed_ds_train_low.map(preprocess_function, num_proc = multiprocessing.cpu_count(), batched=True).rename_column(\"Label\",\"labels\")\n",
    "\n",
    "tokenized_dataset_test = pre_processed_ds_test_low.map(preprocess_function, num_proc = multiprocessing.cpu_count(), batched=True).rename_column(\"Label\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "tokenized_dataset_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# Load the pre-trained model for sequence classification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(os.getenv(\"MODEL_NAME\"), device_map = \"cuda\", num_labels=2)  # Binary classificatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import wandb\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.getenv(\"WANDB_NAME\"),  # Output directory\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate every epoch\n",
    "    learning_rate=5e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=5,               # Number of epochs\n",
    "    weight_decay=0.01,              # Weight decay\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=os.getenv('WANDB_NAME'),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,  # Training data\n",
    "    eval_dataset=tokenized_dataset_test,   # Evaluation data\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'model_name': os.getenv(\"WANDB_NAME\"),\n",
    "    'finetuned_from': os.getenv('MODEL_NAME'),\n",
    "    'dataset': os.getenv(\"DATASET\")\n",
    "}\n",
    "\n",
    "tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "# Load the fine-tuned model and tokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(os.getenv(\"WANDB_NAME\"), device_map = \"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.getenv(\"WANDB_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"No Vulnerability\",\n",
    "    \"LABEL_1\": \"Vulnerability Detected\"\n",
    "}\n",
    "\n",
    "# Create a text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example: Classify a single text input\n",
    "text = \"Detected SQL injection attempt on website query /login.php\"\n",
    "\n",
    "result = classifier(text)\n",
    "result[0]['label'] = label_map[result[0]['label']]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify multiple texts\n",
    "\n",
    "texts = [\n",
    "    \"SSL certificate expired on service hosted at 113.119.19.172, accessed by 219.46.175.211\",\n",
    "    \"Traffic from 1.1.1.1 to 8.8.8.8 on port 80 detected as normal.\",\n",
    "    \"Worm spreading detected in subnet 21.10.2.152/24\",\n",
    "    \"ARP spoofing attempt observed on network involving IP 49.25.235.140\",\n",
    "    \"Multiple packets dropped due to suspicious activity from IP 12.5.10.1\",\n",
    "    \"Traffic from 219.237.107.94 to 1.1.1.1 on port 20586 detected as normal.\",\n",
    "    \"Detected SQL injection attempt on website query /pog.php\",\n",
    "    \"Traffic from 219.237.107.94 to 98.163.156.42 on port 440 detected as normal.\",\n",
    "    \"Traffic from 219.237.107.94 to 98.163.156.42 on port 20586 detected as normal.\",\n",
    "    \"Traffic from 219.237.107.94 to 98.163.156.42 on port 8000 detected as normal.\",\n",
    "    \"Port scanning activity from IP 90.59.147.123 detected targeting 17.185.148.183\"\n",
    "]\n",
    "\n",
    "results = classifier(texts)\n",
    "\n",
    "# Apply label mapping to the results\n",
    "for result in results:\n",
    "    result['label'] = label_map[result['label']]\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "true_labels = [\n",
    "    \"Vulnerability Detected\",  # For \"Detected SQL injection attempt on website query /login.php\"\n",
    "    \"No Vulnerability\",          # For \"Normal traffic from 192.168.0.1 to 192.168.0.10 on port 443\"\n",
    "    \"Vulnerability Detected\",\n",
    "    \"Vulnerability Detected\",\n",
    "    \"Vulnerability Detected\",\n",
    "    \"No Vulnerability\",\n",
    "    \"Vulnerability Detected\",\n",
    "    \"No Vulnerability\",\n",
    "    \"No Vulnerability\",\n",
    "    \"No Vulnerability\",\n",
    "    \"Vulnerability Detected\"# For \"Multiple packets dropped due to suspicious activity from IP 10.0.0.1\"\n",
    "]\n",
    "\n",
    "predicted_labels = [result['label'] for result in results]\n",
    "predicted_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
